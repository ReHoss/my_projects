{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monde grille 4x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# These commands allow to modify a .py script and directly obtain changes, whitout restarting kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Contains implementation for the four transition matrix\n",
    "import transition_matrix\n",
    " # DP algorithms we have implemented\n",
    "from dynamic_programming import policy_iteration, value_iteration, mc_state_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the starting point is the origin $(0, 0)$, the state $(3, 2)$ rewards $+1$, the state $(3, 1)$ rewards $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining global variables\n",
    "\n",
    "# States are defined by tuples\n",
    "LIST_STATES = [(i, j) for i in range(4) for j in range(3)]\n",
    "LIST_STATES.remove((1, 1))\n",
    "LIST_ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "# List of transition matrix functions\n",
    "LIST_TRANSITION_MATRIX = [transition_matrix.transition_matrix_up,\n",
    "                          transition_matrix.transition_matrix_down,\n",
    "                          transition_matrix.transition_matrix_left,\n",
    "                          transition_matrix.transition_matrix_right]\n",
    "\n",
    "# Building bijection between states and integers\n",
    "DICT_STATES = {key: value for (key, value) in enumerate(LIST_STATES)}\n",
    "DICT_STATES.update(dict([reversed(i) for i in DICT_STATES.items()]))\n",
    "\n",
    "# List of non-final states\n",
    "LIST_NON_FINAL_STATES = list(LIST_STATES)\n",
    "LIST_NON_FINAL_STATES.remove((3, 1))\n",
    "LIST_NON_FINAL_STATES.remove((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 0),\n",
       " 1: (0, 1),\n",
       " 2: (0, 2),\n",
       " 3: (1, 0),\n",
       " 4: (1, 2),\n",
       " 5: (2, 0),\n",
       " 6: (2, 1),\n",
       " 7: (2, 2),\n",
       " 8: (3, 0),\n",
       " 9: (3, 1),\n",
       " 10: (3, 2),\n",
       " (0, 0): 0,\n",
       " (0, 1): 1,\n",
       " (0, 2): 2,\n",
       " (1, 0): 3,\n",
       " (1, 2): 4,\n",
       " (2, 0): 5,\n",
       " (2, 1): 6,\n",
       " (2, 2): 7,\n",
       " (3, 0): 8,\n",
       " (3, 1): 9,\n",
       " (3, 2): 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT_STATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show all row of transitions matrix when action is downward sums to $1$. Other matrix also returns $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition from state (0, 0)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (0, 1)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (0, 2)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (1, 0)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (1, 2)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (2, 0)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (2, 1)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (2, 2)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n",
      "Transition from state (3, 0)\n",
      "Sum of transitions probabilities: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To check other matrix, choose one matrix from LIST_TRANSITION_MATRIX\n",
    "for tuple_state in LIST_NON_FINAL_STATES:\n",
    "    sum_proba = 0\n",
    "    for tuple_new_state in LIST_STATES:\n",
    "        sum_proba += transition_matrix.transition_matrix_down(tuple_state,\n",
    "                                                      tuple_new_state)\n",
    "\n",
    "    print(f'Transition from state {tuple_state}')\n",
    "    print(f'Sum of transitions probabilities: {sum_proba}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy and value iteration algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the optimal policies with respect to different reward values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_REWARD = [-10, -1, -0.04, 0, 1, 10]\n",
    "\n",
    "# Initialise value fonction and policy function (represented as arrays)\n",
    "ARRAY_V = np.zeros(len(LIST_STATES))\n",
    "ARRAY_V[DICT_STATES[(3, 1)]] = -1\n",
    "ARRAY_V[DICT_STATES[(3, 2)]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialiaze a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|↑|↑|↑| |\n",
      "|↑|X|↑| |\n",
      "|↑|↑|↑|↑|\n"
     ]
    }
   ],
   "source": [
    "# Initialise policy\n",
    "ARRAY_POLICY = np.zeros(len(LIST_NON_FINAL_STATES),\n",
    "                        dtype=int)\n",
    "dynamic_programming.print_policy(ARRAY_POLICY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for reward r = -10\n",
      "|→|→|→| |\n",
      "|↑|X|→| |\n",
      "|→|→|→|↑|\n",
      "\n",
      "Optimal policy for reward r = -1\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|→|→|↑|↑|\n",
      "\n",
      "Optimal policy for reward r = -0.04\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|↑|→|↑|←|\n",
      "\n",
      "Optimal policy for reward r = 0\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|↑|←|↑|←|\n",
      "\n",
      "Optimal policy for reward r = 1\n",
      "|→|→|←| |\n",
      "|↑|X|←| |\n",
      "|↑|→|←|↓|\n",
      "\n",
      "Optimal policy for reward r = 10\n",
      "|→|→|←| |\n",
      "|↑|X|←| |\n",
      "|↑|→|←|↓|\n",
      "\n",
      "CPU times: user 119 ms, sys: 7.83 ms, total: 127 ms\n",
      "Wall time: 97.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Policy iteration\n",
    "for reward in LIST_REWARD:\n",
    "    print(f'Optimal policy for reward r = {reward}')\n",
    "    array_optim_policy = policy_iteration(ARRAY_POLICY,\n",
    "                                          ARRAY_V,\n",
    "                                          gamma=0.9,\n",
    "                                          r=reward)\n",
    "    dynamic_programming.print_policy(array_optim_policy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|↑|↑|↑| |\n",
      "|↑|X|↑| |\n",
      "|↑|↑|↑|↑|\n"
     ]
    }
   ],
   "source": [
    "# Initialise policy\n",
    "ARRAY_POLICY = np.zeros(len(LIST_NON_FINAL_STATES),\n",
    "                        dtype=int)\n",
    "dynamic_programming.print_policy(ARRAY_POLICY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for reward r = -10\n",
      "|→|→|→| |\n",
      "|↑|X|→| |\n",
      "|→|→|→|↑|\n",
      "\n",
      "Optimal policy for reward r = -1\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|→|→|↑|↑|\n",
      "\n",
      "Optimal policy for reward r = -0.04\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|↑|→|↑|←|\n",
      "\n",
      "Optimal policy for reward r = 0\n",
      "|→|→|→| |\n",
      "|↑|X|↑| |\n",
      "|↑|←|↑|←|\n",
      "\n",
      "Optimal policy for reward r = 1\n",
      "|→|→|←| |\n",
      "|↑|X|←| |\n",
      "|↑|→|←|↓|\n",
      "\n",
      "Optimal policy for reward r = 10\n",
      "|→|→|←| |\n",
      "|↑|X|←| |\n",
      "|↑|→|←|↓|\n",
      "\n",
      "CPU times: user 261 ms, sys: 56.6 ms, total: 317 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Value iteration\n",
    "for reward in LIST_REWARD:\n",
    "    print(f'Optimal policy for reward r = {reward}')\n",
    "    array_optim_policy = value_iteration(ARRAY_POLICY,\n",
    "                                         ARRAY_V,\n",
    "                                         gamma=0.9,\n",
    "                                         r=reward)\n",
    "    dynamic_programming.print_policy(array_optim_policy)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations using the optimal policy $\\pi^*$:\n",
    "- $r = -1 $ or $ -10$, the agent wants to reach a final state as soon as possible, even if the final state is rewarding negatively.\n",
    "\n",
    "\n",
    "- $r = -0.04$, the agent wants the most rewarding final state as soon as possible because the moving cost (negative reward) is quite cheap -close to zero-. Note that the agent take the risk to terminate in the negative final state (by going to right in $(1, 0)$), since he wants to reach the end fastly.\n",
    "\n",
    "\n",
    "- $r = 0$, the agent wants the most rewarding final state passing by the up part of the grid ! Indeed the $(1, 0)$ action is **left** because each non terminal state has a cost of zero, there is no moving penalty.\n",
    "\n",
    "\n",
    "- $r = 1$ or $10$, each non terminal state is rewarding, hence the optimal strategy is to never reach a terminal state. Note that the optimal action in $(3, 1)$ is downward since it ensure to never reach the bad terminal state located in $(3, 1)$.\n",
    "\n",
    "With our implementation, policy iteration is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo first and every visit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the optimal policy $\\pi^*$ for a reward parameter $r = -0.04$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a simulation of one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Environment:\n",
      "[[ 0  0  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 1  0  0  0]]\n",
      "-----------------------\n",
      "Action choosed by agent: UP\n",
      "Action after perturbation: UP\n",
      "Environment:\n",
      "[[ 0  0  0  0]\n",
      " [ 1 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: UP\n",
      "Action after perturbation: RIGHT\n",
      "Environment:\n",
      "[[ 0  0  0  0]\n",
      " [ 1 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: UP\n",
      "Action after perturbation: UP\n",
      "Environment:\n",
      "[[ 1  0  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: RIGHT\n",
      "Action after perturbation: UP\n",
      "Environment:\n",
      "[[ 1  0  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: RIGHT\n",
      "Action after perturbation: RIGHT\n",
      "Environment:\n",
      "[[ 0  1  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: RIGHT\n",
      "Action after perturbation: RIGHT\n",
      "Environment:\n",
      "[[ 0  0  1  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: -0.04\n",
      "-----------------------\n",
      "-----------------------\n",
      "Action choosed by agent: RIGHT\n",
      "Action after perturbation: RIGHT\n",
      "Environment:\n",
      "[[ 0  0  0  1]\n",
      " [ 0 -1  0  0]\n",
      " [ 0  0  0  0]]\n",
      "Reward received: 1\n",
      "-----------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RL_maze.Agent at 0x7f89f160ad10>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Episod simulation under optimal policy\n",
    "dynamic_programming.episode_simulation(ARRAY_POLICY_OPTIM,\n",
    "                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal policy\n",
    "ARRAY_POLICY_OPTIM = dynamic_programming.policy_iteration(ARRAY_POLICY,\n",
    "                                                          ARRAY_V,\n",
    "                                                          gamma=0.9,\n",
    "                                                          r=-0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal values with first visit MC\n",
    "ARRAY_V_OPTIM_FIRST = dynamic_programming.mc_state_evaluation(ARRAY_POLICY_OPTIM,\n",
    "                                                             ARRAY_V,\n",
    "                                                             str_mode='first',\n",
    "                                                             n_step=10000,\n",
    "                                                             gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|0.61|0.77|0.93|1.00|\n",
      "|0.49|XXXXX|0.58|-1.00|\n",
      "|0.37|0.31|0.41|0.14|\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "dynamic_programming.print_values(ARRAY_V_OPTIM_FIRST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal values with first visit MC\n",
    "ARRAY_V_OPTIM_EVERY = dynamic_programming.mc_state_evaluation(ARRAY_POLICY_OPTIM,\n",
    "                                                             ARRAY_V,\n",
    "                                                             str_mode='every',\n",
    "                                                             n_step=10000,\n",
    "                                                             gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|0.61|0.77|0.93|1.00|\n",
      "|0.48|XXXXX|0.56|-1.00|\n",
      "|0.37|0.31|0.40|0.17|\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "dynamic_programming.print_values(ARRAY_V_OPTIM_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : the state value $(3, 0)$ is hard to estimate since there are few episode including this state when choosing the optimal policy $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both algorithm seem to converge to the same action values, it can be shown *every visit* is faster (convergence rate)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
